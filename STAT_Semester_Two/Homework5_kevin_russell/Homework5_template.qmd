---
title: "Homework 5"
author: Kevin Russell
format: 
  html:
    embed-resources: true
---


This homework will involve data from transcripts of the American television show "The Office" which aired on NBC from 2005 to 2013. All transcripts are available in the R package schrute, which is available on CRAN.

```{r}
library(schrute)
library(tidyverse)
data(theoffice)
glimpse(theoffice)
```

Note that each row in the data frame corresponds to a line spoken by a character in the show. 

# Problem 1 [60 Points Total]

## Part A [10 Points]

The column named text in theoffice contains the words spoken by a character in a line of dialog. Recreate the following figure showing the total number of words spoken in each episode of each season.

```{r}
library(tidytext)
office_tidy <- theoffice |>
  unnest_tokens(word, 
                text)

office_tidy |>
  glimpse()
```
```{r}
word_counts <- office_tidy |>
  group_by(season, episode) |>
  count()

word_counts <- word_counts |>
  mutate(season = paste("Season", season))

word_counts |>
  ggplot(aes(x = episode,
         y = n)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  facet_wrap(~season) +
  labs(y = "Number of Words", x = "Epsiode Number")
```


Based on the number of words, list out the episodes do you think might be "double episodes" (42 minute instead of 21 long)? 

```{r}
double <- word_counts |>
  filter(n >= 4500) |>
  dplyr::select(season, episode) |>
  # This is an episode with a lot of words, but not a double episode
  filter(season != "Season 9" | episode != "16")

double
```

Verify your claims by showing that none of these episodes have an episode number right after them. (Example, there is no Season 4 Episode 2, confirming Season 4 Episode 1 is a double episode)

```{r}
seasons <- parse_number(double$season)
episodes <- double$episode
new_df <- data.frame(season = seasons, episode = episodes + 1)

office_tidy |>
  inner_join(new_df, by=c("season", "episode"))
```

Finally, guess which episode (hint: in Season 6) was a clip show. Feel free to verify using wikipedia 

```{r}
#S6E14 seems to be the clip show, as it has very few words compared to other episodes.
```


## Part B [8 Points]

Create a word cloud of the 50 most-used words across the entire series, excluding stop words.

```{r}
data("stop_words")

office_counts <- office_tidy |>
  summarize(term_frequency = n(),
            .by = c(word)) |>
  arrange(desc(term_frequency))
```

```{r}
library(ggwordcloud)

office_clean <- office_counts |> 
  anti_join(stop_words,
            by = join_by(word))


office_clean |> 
  slice_max(term_frequency, 
            n = 50) |>
  ggplot() +
  geom_text_wordcloud_area(aes(label = word, 
                               size = term_frequency)) +
  scale_size_area(max_size = 30) 
```

What word was used the most?

```{r}
office_clean |>
  slice(1)
# Yeah- 2930 instances
```

## Part C [4 Points]

Recreate the following word cloud of names of the 30 characters from the office who spoke the most words throughout the series. The size of the name should be proportional to how many words [including stop words] that the character spoke.

```{r}
names <- office_tidy |>
  count(character) |>
  arrange(desc(n))

names |> 
  slice_max(n, 
            n = 30) |>
  ggplot() +
  geom_text_wordcloud_area(aes(label = character, 
                               size = n)) +
  scale_size_area(max_size = 50) +
  theme_bw()
```

## Part D [8 Points]

Consider the names of the 30 characters chosen for Part C. Recreate the following bar chart. It shows how many times that each character's name appeared as spoken dialog throughout the show's run.

```{r}
names <- names |>
  mutate(word = character) |>
  mutate(word = tolower(word))

names30 <- names |>
  arrange(desc(n)) |>
  head(30)

test <- office_clean |>
  inner_join(names30, by = "word") |>
  dplyr::select(word, term_frequency) |>
  distinct(word, .keep_all = TRUE) |>
  arrange(desc(term_frequency))

test |>
  ggplot(aes(x = term_frequency,
             y = reorder(word, term_frequency))) +
  geom_bar(stat = "identity") +
  labs(x = "Mentions in Show",
       y = "Character Name")
```

## Part E [10 Points]

Let's see if there are other common first names that are spoken throughout the show.

To do this, determine the 1500 most popular names in the babynames dataset [package babynames], remove any names that are also stop words (like Will and May), then count how times each of these names are spoken in the office.

Once you've done that, use the results to recreate the following plot. Each bar is colored based on whether or not the names appeared in Part D.

```{r}
library(babynames)

babynames_group <- babynames |>
  group_by(name)

stop_words_name <- stop_words |>
  mutate(name = word)

sum_names <- babynames_group |>
  summarize(count_names = sum(n)) |>
  arrange(desc(count_names)) |>
  head(1500) |>
  mutate(name = tolower(name)) |>
  anti_join(stop_words_name, by = "name") |>
  mutate(word = name) |>
  dplyr::select(word)
```

```{r}
spoken_names <- office_counts |>
  inner_join(sum_names, by="word") |>
  arrange(desc(term_frequency)) |>
  head(40)

spoken_names <- spoken_names |>
  mutate(in_names = ifelse(word %in% names30$word, 1, 0)) |>
  mutate(in_names = as.factor(in_names))

spoken_names |>
  ggplot(aes(x = term_frequency,
             y = reorder(word, term_frequency),
             fill = in_names)) +
  geom_bar(stat = "identity") +
  labs(x = "Mentions in Show",
       y = "Person Name") +
  theme_bw() +
  theme(legend.position = "none")
```


Which character names shown in part D are missing from our list of common baby names?

```{r}
names30 |>
  mutate(in_baby_names = ifelse(word %in% sum_names$word, 1, 0)) |>
  filter(in_baby_names == 0)

# Gabe, Creed, Deangelo
```

## Part F [6 Points]

In the office script, the writers often like to spell a character screaming or laughing using words that are at least two letters long and composed entirely of h's and a's. Approximately recreate the following word cloud demonstrating all of these words, with size proportional often they are used.

```{r}
ah <- office_counts |>
  filter(grepl("^([ah]{2,})$", word))

ah |> 
  ggplot() +
  geom_text_wordcloud_area(aes(label = word, 
                               size = term_frequency)) +
  scale_size_area(max_size = 80) +
  theme_bw()
```

## Part G [8 points]

Sometimes, the writers like to indicate yelling by showing the same vowel repeated four or more times in a row (e.g. aaaa, iiii, oooo) in a word. For example, "staaaarrrrted".

Recreate the following plot depicting how times this occurs in episodes written by each writer.

```{r}
office_vowels <- office_tidy |>
  filter(grepl("(a{4,}|e{4,}|i{4,}|o{4,}|u{4,}|y{4,})", word))
```

```{r}
office_vowels <- office_vowels |>
    separate_longer_delim(writer, 
                        delim = ";") |>
  count(writer) |>
  arrange(desc(n))
```

```{r}
office_vowels |>
  ggplot(aes(y = reorder(writer, n), x = n)) +
  geom_bar(stat = "identity") +
  labs(x = "Number of tiiiimes",
       y = "Writer") +
  theme_bw()
```

## Part H [6 points]

Treating each season as a document, calculate the tf_idf scores for the words. Which word has the highest tf_idf score for each season?

```{r}
office_season_counts <- office_tidy |>
  summarize(term_frequency = n(),
            .by = c(word, season)) |>
  arrange(desc(term_frequency))
```


```{r}
idf_by_season <- office_season_counts |> 
  summarize(term_frequency = sum(term_frequency),
            .by = c(season, word)) |>
  bind_tf_idf(word, 
              season, 
              term_frequency) |> 
  arrange(desc(tf_idf))

head(idf_by_season)
```


# Problem 2 [40 Points Total]

## Part A [8 Points]

Create a document term matrix corresponding to the number of times each character speaks in each episode. Each character name (e.g. Michael, Dwight, Pam) should correspond to a "term". Each episode corresponds to a "document". 

```{r}
library(tm)

office_episode_counts <- theoffice |>
  summarize(character_frequency = n(),
            .by = c(character, season, episode)) |>
  arrange(desc(character_frequency)) |>
    mutate(ep = str_c(season,
                              episode,
                              sep = "_"))

document_term_matrix <- office_episode_counts |> 
  cast_dtm(document = ep, 
           term = character,
           value = character_frequency)

document_term_matrix
```

Hint: Your matrix should consist of 186 documents and 773 terms.

## Part B [8 Points]

Use latent Dirichlet allocation to fit a topic model with four topics to this data.

```{r}
library(topicmodels)
set.seed(3)
episodes_lda <- LDA(document_term_matrix,
                    k = 4, 
                    control = list(seed = 3))

episodes_lda |>
  save(file = "episodes_lda.Rda")
```

## Part C [12 Points]

Create a graphic showing the beta scores for the top 10 characters for each topic. 

```{r}
load("episodes_lda.Rda")
episode_topics <- episodes_lda |>
  tidy(matrix = "beta")

top_terms <- episode_topics |>
  group_by(topic) |>
  slice_max(beta, n = 10) |> 
  mutate(topic = factor(topic))

top_terms |>
  ggplot(aes(beta, 
             term, 
             fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, 
             scales = "free_y", 
             ncol = 4) 
```

Is there a character whose prevalence seems to shift the most between topics?

Michael has a very high beta value for 1 and 4 but is not among the top 10 characters for 2 or 3.



## Part D [12 Points]

Create a graphic showing the distribution of topics for each episode across seasons. 

```{r}
episode_memberships <- tidy(episodes_lda, 
                            matrix = "gamma")

topic_names <- c("1",
                 "2",
                 "3",
                 "4")

episode_memberships <- episode_memberships |> 
  separate_wider_delim(document,
                       delim = "_",
                       names = c("season",
                                 "episode")) |>
  mutate(season = parse_number(season),
         episode = parse_number(episode),
         topic = as_factor(topic_names[topic]))

episode_memberships |> 
  arrange(topic) |>
  ggplot() +
  geom_col(aes(x = episode,
                y = gamma,
                fill = topic),
           position = "fill", 
           color= "black", 
           linewidth = 0.1) +
  facet_wrap(~season, 
             scales = "free_x") +
  theme(legend.position = "top",
    strip.background = element_blank(),
    strip.text.x = element_blank())
```

Does the topic distributions seem to shift as the series progresses? How so? What do you think is driving this change?

Yes. Seasons 1-5 are largely comprised of topics 1 and 4, which are the topics that include Michael. Season 6 marks the start of a shift away from those two topics, and toward topics 2 and 3, which include Andy more strongly. In this way, we can see how Michael's departure is reflected by the changing of these topics.