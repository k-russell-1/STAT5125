---
title: "Takehome Final Exam"
author: Kevin Russell
format: 
  html:
    embed-resources: true
editor_options: 
  chunk_output_type: console
---


# Problem 1: [26 Points Total]

On HuskyCT, I have provided a file called fashion_final.csv. This file contains a subset of the "fashion MNIST" data.  This dataset is composed of 1000 rows and 786 columns. Each row represents a different 28 by 28 pixel image of a garment. 

The following is a brief description of the data:

The first column (picture_num) indexes the the photos from 1 to 1000. The second column (garment) is categorical, taking on 10 different values (0-9) to describe different classes of garment. The remaining columns all have names of the form x.._y.. describing the pixel intensity of the image at a particular location. For instance, x14_y14 describes the pixel intensity at the x=14 and y=14 coordinate, near the center of each image. Meanwhile, the x1_y28 variable describes the pixel intensity at the bottom right of each image.

Through the various parts of this problem, we read in, clean, manipulate, and plot this data. I have included reference datasets/images to compare against to check if you performed the steps correctly.

## Part A: Read and Clean the Data [4 Points]

Read in the data from fashion_final.csv. Change the garment variable from numeric to a factor. Relabel the factors according to the following levels.

- 0 T-shirt/top
- 1 Trouser
- 2 Pullover
- 3 Dress
- 4 Coat
- 5 Sandal
- 6 Shirt
- 7 Sneaker
- 8 Bag
- 9 Ankle boot

Once you have completed these steps, your new data frame should match the one provided on HuskyCT as "fashion_clean.Rda". Recreate the following bar chart showing the number of each garment appearing in the dataset.

```{r}
library(tidyverse)
theme_set(theme_bw())
```


```{r}

fashion <- read.csv("fashion_final.csv")

fashion <- fashion |>
  mutate(garment = ifelse(garment == 0, "T-shirt/top", garment)) |>
  mutate(garment = ifelse(garment == 1, "Trouser", garment)) |>
  mutate(garment = ifelse(garment == 2, "Pullover", garment)) |>
  mutate(garment = ifelse(garment == 3, "Dress", garment)) |>
  mutate(garment = ifelse(garment == 4, "Coat", garment)) |>
  mutate(garment = ifelse(garment == 5, "Sandal", garment)) |>
  mutate(garment = ifelse(garment == 6, "Shirt", garment)) |>
  mutate(garment = ifelse(garment == 7, "Sneaker", garment)) |>
  mutate(garment = ifelse(garment == 8, "Bag", garment)) |>
  mutate(garment = ifelse(garment == 9, "Ankle Boot", garment)) |>
  mutate(garment = as.factor(garment))

library(ggplot2)

fashion |>
  group_by(garment) |>
  summarize(count = n()) |>
  ggplot(aes(x = count, y = reorder(garment, desc(garment)))) + 
  geom_bar(stat = "identity") +
  labs(x = "count", y = "garment")
```

## Part B: Tidying the Data [8 Points]

Apply data manipulation techniques such that each row in the data frame corresponds to a single pixel of a single image. This new version of the data should have 784000 rows and five columns, occurring the following order:

- garment---a factor describing the type of garment (e.g. Trouser)
- picture_num---a number describing which photo the pixel is part of (spanning 1-1000)
- x---a number describing the horizontal coordinate of the pixel (spanning 1-28)
- y---a number describing the vertical coordinate of the pixel (spanning 1-28)
- value---a number describing the image's pixel intensity at this pixel.

When completed, your new data frame should match the one provided on HuskyCT as "fashion_clean_longer.Rda".

```{r}
fashion_clean_longer <- fashion |>
  pivot_longer(cols=starts_with("x"), names_to = "x", values_to = "value") |>
  mutate(x = parse_number(x),
         y = rep(rep(28:1, each = 28), 1000))
```

## Part C: Creating a Data frame of Example Images [4 Points]

Create a new data frame called fashion_examples. fashion_examples should be a subset of fashion_clean_longer containing the info for exactly one of each type of garment. Since there are ten types of garments and each image has 784 pixels, fashion_examples should contain exactly 7840 rows. 

```{r}
fashion_1 <- fashion_clean_longer |>
  filter(garment == "T-shirt/top") |>
  head(784)

fashion_2 <- fashion_clean_longer |>
  filter(garment == "Trouser") |>
  head(784)

fashion_3 <- fashion_clean_longer |>
  filter(garment == "Pullover") |>
  head(784)

fashion_4 <- fashion_clean_longer |>
  filter(garment == "Dress") |>
  head(784)

fashion_5 <- fashion_clean_longer |>
  filter(garment == "Coat") |>
  head(784)

fashion_6 <- fashion_clean_longer |>
  filter(garment == "Sandal") |>
  head(784)

fashion_7 <- fashion_clean_longer |>
  filter(garment == "Shirt") |>
  head(784)

fashion_8 <- fashion_clean_longer |>
  filter(garment == "Sneaker") |>
  head(784)

fashion_9 <- fashion_clean_longer |>
  filter(garment == "Bag") |>
  head(784)

fashion_10 <- fashion_clean_longer |>
  filter(garment == "Ankle Boot") |>
  head(784)

fashion_examples = rbind(fashion_1, fashion_2, fashion_3, fashion_4, fashion_5, fashion_6, fashion_7, fashion_8, fashion_9, fashion_10)
```

Note: There are many possible correct responses to this question. It depends on which examples you pick.

## Part D: Plotting Example Images [4 Points]

Use geom_raster to make a faceted plot like one below showing one of each kind of garment. Note---your graphic doesn't have to exactly match the one shown below. It depends on which example garments you picked.

```{r}
fashion_examples |>
  ggplot(aes(x = x,
             y = y,
             fill = value)) + 
  geom_raster() +
  scale_fill_gradient(low = "white", 
                      high = "black")  +
  coord_fixed() + 
  facet_wrap(~ garment)
```


## Part E: Computing the Average Garments [4 Points]

Use the dataset fashion_clean_longer (created in part B) to create a data frame showing the average value across the data for each pixel coordinate and each type of garment. Give this data frame the name fashion_average. It should match the data frame provided in "fashion_average.Rda".

```{r}
fashion_average <- fashion_clean_longer |> 
  group_by(garment, x, y) |>
  summarise(value = mean(value))
```

Which type of garment has the highest average pixel intensity at coordinate x = 10, y = 10?

```{r}
fashion_average |>
  filter(x == 10, y == 10) |>
  arrange(desc(value)) |>
  head(1)
```

Coat.

## Part F: Plot the Averages [2 Points]

Use geom_raster to make a faceted plot like the one below showing the average pixel intensities for each type of garment.

```{r}
fashion_average |>
  ggplot(aes(x = x,
             y = y,
             fill = value)) + 
  geom_raster() +
  scale_fill_gradient(low = "white", 
                      high = "black")  +
  coord_fixed()+ 
  facet_wrap(~ garment)
```


# Problem 2: [42 Points Total]

On HuskyCT, I have provided two files related to books. 

- book_names.csv contains the title and goodreads book id for 10,000 popular books. 

- book_ratings.csv contains the average good reads ratings and original publication year for each of these 10,000 books, along with their goodreads book id and the code for the language of the book.

In this question, we will be reading in, merging, and analyzing this dataset.

## Part A: Read and Merge Data [6 points]

Read in both book_names.csv and book_ratings.csv. Merge the two datasets based on the goodreads book id to create a single data frame showing book title, original publication year, average rating for each book, and language_code. Remove any entries with missing book names. 

Also remove any books with titles in which the first character is not one of A-Z, a-z, or 0-9. Convert language_code into a factor.

```{r}
names <- read.csv("book_names.csv")
ratings <- read.csv("book_ratings.csv")

books <- names |>
  mutate(book_id = goodreads_book_id) |>
  dplyr::select(book_title, book_id) |>
  inner_join(ratings, by= "book_id") |>
  drop_na("book_title") |>
  filter(grepl("^[A-Za-z0-9]", book_title)) |>
  dplyr::select(book_title, language_code, original_publication_year, average_rating) |>
  mutate(language_code = as.factor(language_code))
```

The result should have 9138 rows and 4 columns, matching the data frame provided in books.Rda on HuskyCT.

## Part B: Creating New Variables [8 points]

Add seven additional variables to the data frame from part A:

- num_words: a numeric variable measuring the number of words in the book's title.
- num_vowels: a numeric variable measuring the number of vowels in the book's title.
- num_consonants: a numeric variable measuring the number of consonants in the book's title.
- num_digits: a numeric variable measuring the number of times the digits 0-9 occur in the book's title.
- num_good: a numeric variable measuring the number of times the word "good" appears in the book's title.
- num_bad: a numeric variable measuring the number of times the word "bad" appears in the book's title.
- new_millennium: a logical variable indicating whether the book was published in the year 2000 or later.

Take care to account for both upper and lower case letters. 

```{r}
books_df <- books |>
  mutate(title_lower = tolower(book_title)) |>
  mutate(num_words = str_count(title_lower, ' ') + 1) |>
  mutate(num_vowels = str_count(title_lower, 'a|e|i|o|u')) |>
  # y will be a consonant
  mutate(num_consonants = str_count(title_lower, 'b|c|d|f|g|h|j|k|l|m|n|p|q|r|s|t|v|w|x|y|z')) |>
  mutate(num_digits = str_count(title_lower, '0|1|2|3|4|5|6|7|8|9')) |>
  mutate(num_good = str_count(title_lower, 'good')) |>
  mutate(num_bad = str_count(title_lower, 'bad')) |>
  mutate(new_millenium = ifelse(original_publication_year >= 2000, TRUE, FALSE)) |>
  dplyr::select(-title_lower)
```

How many books' titles contain the both words "good" and "bad"?

```{r}
books_df |>
  filter(num_good >= 1) |>
  filter(num_bad >= 1) |>
  length()

# 11
```

Recreate the following bar chart summarizing the number of books having titles starting with each character. Convert any lower case starting letters to upper case.

```{r}
books_df |>
  mutate(upper_title = toupper(book_title)) |>
  mutate(starting_character = substr(upper_title, 1, 1)) |>
  group_by(starting_character) |>
  summarise(count = n()) |>
  ggplot(aes(x = count, y = starting_character)) +
  geom_bar(stat = "identity")
```

Also, make plots similar to the following exploring the data and its missingness pattern.

```{r}
library(visdat)
books_df |>
  vis_dat()
```


```{r}
books_df |>
  vis_miss()
```

Which variables appear to have missing data?

Only language code appears to have missing data.

## Part C: Comparing Book Ratings before and after Y2K [6 Points]

Is the mean average book score higher for books published before the new millennium, or after the new millennium? Use a formal hypothesis test to support your answer.

```{r}
post <- books_df |>
  filter(original_publication_year >= 2000) |>
  dplyr::select(average_rating) |>
  unlist()

pre <- books_df |>
  filter(original_publication_year < 2000) |>
  dplyr::select(average_rating) |>
  unlist()

# Null hypothesis: The means are not different in average rating between books published before 2000 and in 2000 and after. Alternative hypothesis: The means are different.

t.test(pre,post)

# Very small p-value tells us to reject the null hypothesis, and conclude that the means are indeed different.
```

Moreover, does the full distribution (not just the mean) of average book ratings differ between those books released on or before the year 2000 versus those published after the year 2000? For this part, conduct a formal hypothesis test and provide p-value to support your claim.

```{r}
# Null hypothesis: The distributions are not different in average rating between books published before 2000 and in 2000 and after. Alternative hypothesis: The distributions are different.

ks.test(pre, post)

# Very small p-value tells us to reject the null hypothesis, and conclude that the distributions are indeed different.
```


## Part D: Test Train Split [2 Points]

Perform a test-train split of the augmented dataset created in Part B such that 90 percent of the data is included in the training set.

```{r}
library(tidymodels)

set.seed(12345)
books_split <- initial_split(books_df, 
                              prop = 0.9)

books_train <- books_split |>
  training()

books_test <- books_split |>
  testing()
```

## Part E: Fitting a Regression Model [8 points]

Using the tidymodels framework, fit a Lasso regression model to the training data with a penalty term valued at 0.0002. Use average_rating as the response variable, and the following covariates:

- num_words
- num_vowels
- num_consonants
- num_digits
- num_good
- num_bad
- language_code
- a natural spline around publication year with 4 degrees of freedom.

Impute any missing data using 5-nearest neighbors. For the language_code factor, create an "other" category for any languages that make up less 0.5 percent of the data. Normalize all the numeric predictors before fitting your model. Hint: remember to that the step_dummy() for the language code variable should occur after the imputation and other category steps.



```{r}
fit_glmnet <- linear_reg(penalty = 0.0002) |>
  set_engine("glmnet") |> 
  set_mode("regression")

recipe_books <- recipe(data = books_train,
                      formula = average_rating ~ num_words + num_vowels + num_consonants + num_digits + num_good + num_bad + language_code + original_publication_year) |>
  step_impute_knn(all_predictors()) |>
  step_ns(original_publication_year, deg_free = 4)

recipe_books <- recipe_books |>
  step_other(language_code, 
             threshold = 0.005,
             other = "other") |>
  step_dummy(all_nominal_predictors()) |>
    step_normalize(all_predictors())
```

Display a tidy table showing the estimated regression coefficients.

```{r}
recipe_workflow_glm <- workflow()

recipe_workflow_glm <- recipe_workflow_glm |>
  add_model(fit_glmnet) |>
  add_recipe(recipe_books)

fit_glm <- recipe_workflow_glm |>
  fit(books_train)

fit_glm |>
  tidy()
```


## Part F: Assessing Performance on Test set [6 Points]

Use the fitted model from Part E to compute predictions for each of the books in the test set.

```{r}
test_predictions <- fit_glm |>
  predict(books_test)

test_predictions |>
  glimpse()
```

Which book in the test set has the highest predicted rating?

```{r}
books_test_pred <- books_test |>
  bind_cols(test_predictions)

books_test_pred |>
  arrange(desc(.pred)) |>
  head(1)

# Band of Brothers: E Company, 506th Regiment, 101st Airborne from Normandy to Hitler's Eagle's Nest 
```

Create a scatterplot of the true values versus the predicted values. Color the points according to whether or not the book was published before the new millennium.

```{r}
books_test_pred |>
  ggplot(aes(x = .pred, y = average_rating, color = new_millenium)) +
  geom_point() +
  geom_abline(linetype = "dashed", color = "red") +
  xlim(3, 5) +
  ylim(3, 5) + 
  labs(x = "Prediction", y = "True Value")
```

Does the model seem like a good fit to you?  

It does not. It seems visually to only predict in a narow window very close to 4, when the actual ratings are much more spread out. 

Compute the root mean squared error of the fitted model on the test set.

```{r}
sqrt(mean((books_test_pred$average_rating - books_test_pred$.pred)^2))

#0.262
```


## Part G: Bootstrap [6 Points]

Fit the model described in Part E to the 50 bootstrap samples of the original dataset. Use the bootstrap to construct confidence intervals for all of the regression coefficients. Feel free to use the percentile interval method or t interval method (see Homework 11 if this sounds new to you). Which of the regression coefficients appear to be significantly different from zero?

```{r}
set.seed(11)

control_fit <- control_resamples(extract = tidy)

books_bootstrap <- bootstraps(books_df, 
                             times = 50)

bootstrap_fits <-  recipe_workflow_glm |>
  fit_resamples(books_bootstrap,
                control = control_fit)

bootstrap_coefs <- bootstrap_fits  |>
  select(id, .extracts) |>
  unnest(.extracts) |> 
  unnest(.extracts)

bootstrap_coefs |>
  glimpse()
```


```{r}
bootstrap_std <- bootstrap_coefs |>
  summarize(std.error = sd(estimate),
            estimate = mean(estimate),
            .by = term)

bootstrap_std

# Intercept, num_vowels, num_consonants, num_digits, original_publication_year_ns_3, original_publication_year_ns_4
```


Make a plot demonstrating the confidence intervals for all coefficients other than the intercept. It may resemble the following, but will likely differ somewhat based on random seeds.

```{r}
bootstrap_std |>
  filter(term != "(Intercept)") |>
  ggplot(aes(y = term,
             x = estimate)) + 
  geom_segment(aes(x = estimate - 2 * std.error, 
                   xend = estimate + 2 * std.error,
                   yend = term)) +
  geom_point(aes(x = estimate - 2  * std.error),
             shape = "|") +
  geom_point(aes(x = estimate + 2  * std.error),
             shape = "|") +
  xlim(-0.07, 0.07) +
  geom_vline(xintercept = 0, color = "red") +
  labs(x = "Coefficient Estimate")
```


# Problem 3: [32 Points Total]

Here, we consider data on how long it takes twenty individuals to type each letter in the password ".tie5Roanl". The data are available as typingdata.csv on HuskyCT.

Each observation (row) in typingdata.csv is a record of an individual typing the password ".tie5Roanl". The data was collected across 8 typing sessions for each subject. Each session involved typing the password 50 times. Detailed timing of each keystroke was recorded.

The columns in typingdata.csv are as follows:

- subjectID: a unique numerical identifier for each of the 16 participants.
- sessionIndex: the index of the typing session (1-9)
- rep: the repetition of the password within the session (1-50)

The remaining 31 columns in the data present the timing information for the password.  

- Column names of the form H.key designate how long the named key was held down.
- Column names of the form DD.key1.key2 designate the time from when key1 was pressed to when key2 was pressed. 
- Column names of the form UD.key1.key2 designate the time from when key1 was released to when key2 was pressed. 

Note that UD times can be negative, and that H times and UD times add up to DD times.

```{r}
# YOUR SOLUTION HERE
```

## Part A: Read in Data [4 Points]

Read in the data. Convert subjectID and sessionIndex to factors. The new data frame should be named ``typing``.

```{r}
typing <- read.csv("typing_data.csv")

typing <- typing |>
  mutate(subject = as.factor(subject)) |>
  mutate(sessionIndex = as.factor(sessionIndex))
```

Recreate the following scatterplot showing the relationship between H.period and H.t across individuals and sessions.

```{r}
typing |>
  ggplot(aes(x = H.period, y = H.t, color = sessionIndex)) +
  geom_point() +
  facet_wrap(~ subject) +
  theme(legend.position = "bottom")
```

## Part B: Total typing time for each individual [6 Points]

The total time spent typing the password for each trial is defined as the sum of all the of the DD.key1.key2 variables. Add such a new column called total_time to ``typing`` that represents this information. Note: there are lots of different ways to go about doing this. I will accept any of them. It does not have to be elegant.

```{r}
typing <- typing |>
  mutate(total_time = DD.period.t + DD.t.i + DD.i.e + DD.e.five + DD.five.Shift.r + DD.Shift.r.o + DD.o.a + DD.a.n + DD.n.l + DD.l.Return)
```

Once you have created the column total_time, create the following plot showing how the total typing time for Subject 2 changed across sessions and repetitions.

```{r}
typing |>
  filter(subject == 2) |>
  ggplot(aes(x = rep, y = total_time)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ sessionIndex)
```

## Part C: Average Typing Times [2 Points]

Calculate an average total typing time for each session of each individual. Which individual had the slowest individual session? Which session was it?

```{r}
typing |>
  group_by(sessionIndex, subject) |>
  summarise(avg_time = mean(total_time)) |>
  arrange(desc(avg_time)) |>
  head(1)

# Session 3, Subject 12
```


## Part D: Creating Classification Data [4 Points]

Our final task is to build a classifier to distinguish subject 3's typing patterns from those of the remaining users. 

Start by creating an new data frame called typing_classification based on the ``typing`` data frame we created in Part A. typing_classification should not include the subject, sessionIndex, or total_time columns from typing. 

Instead, it should contain a new column called y. y will be a factor taking on the value 1 if subject 3 is the typist, 0 otherwise. 

```{r}
typing_classification <- typing |>
  mutate(y = ifelse(subject == "3", 1, 0)) |>
  mutate(y = as.factor(y)) |>
  dplyr::select(-subject) |>
  dplyr::select(-sessionIndex) |>
  dplyr::select(-total_time)
```

Once the new data frame typing_classification is created, perform a test-train split such that 75 percent of the data is used for training. Make sure to stratify on the y variable in your split.

```{r}
typing_class_split <- typing_classification |>
  initial_split(prop = 0.75, 
                strata = y)

typing_train <- typing_class_split |>
  training()

typing_test <- typing_class_split |>
  testing()
```

## Part F: K-nearest Neighbor Classification [10 Points]

Fit a K-nearest neighbors classifier for y. Use 10-fold cross validation on the training data to choose an optimal value for the number of neighbors, considering the following number of neighbors (1, 10, 20, 30, 40, ..., 200). Use roc_auc as your performance metric. Ensure to normalize all the predictors in your model.

```{r}
library(kknn)
typing_parsnip_knn <- nearest_neighbor(neighbors = tune("neighbors")) |>
  set_engine("kknn") |>
  set_mode("classification")

workflow_typing <- workflow() |>
  add_model(typing_parsnip_knn) |>
  add_formula(y ~ .) |>
  step_normalize(all_predictors())
```

```{r}
neighbor_grid <- crossing(neighbors = c(1, seq(10, 200, by = 10)))

set.seed(1)
comparison_split <- typing_train |>
  mc_cv(times = 10)

metrics <- metric_set(yardstick::roc_auc)
```

```{r}
tuning <- workflow_typing  |>
  tune_grid(
    resamples = comparison_split,
    metrics = metrics,
    grid = neighbor_grid
  )
```

```{r}
tuning |> 
  collect_metrics()
```


Create the following plot, showing the relationship between number of neighbors and the mean performance metric across folds. Color and shapte the hyperparameter value(s) that show maximum performance. Note, since you are likely using a different seed, your performance may differ slightly from mine.

```{r}
tuning |> 
  collect_metrics() |>
  mutate(top = ifelse(neighbors == 60, TRUE, FALSE)) |>
  mutate(top = as.factor(top)) |>
  ggplot(aes(x = neighbors,
             y = mean,
             color = top,
             shape = top)) +
  geom_point() +
  theme(legend.position = "top") +
  labs(color = "Optimal Value", shape = "Optimal Value", x = "Number of Neighbors", y = "Mean ROC_AUC")
```

Once the optimal value for the penalty is chosen. Fit a k-nearest-neighbor model to the entire training data using the optimal number of neighbors

```{r}
knn_parsnip <- nearest_neighbor(neighbors = 60) |> 
  set_mode("classification") |>
  set_engine("kknn")

knn_workflow <- workflow() |>
  add_model(knn_parsnip) |>
  add_formula(y ~ .) |>
  step_normalize(all_predictors())

fit_knn <- knn_workflow |>
  fit(typing_train)

fit_knn |>
  extract_fit_engine()
```


## Part G: Assessing Performance [6 Points]

Make a confusion matrix and determine the roc_auc performance of the knn model from Part F on the test set.

```{r}
knn_test_predictions <- fit_knn |>
  predict(typing_test) |>
  cbind(typing_test$y) |>
  mutate(y = typing_test$y)

knn_test_predictions |>
  conf_mat(truth = y, estimate = .pred_class)
```


```{r}
knn_test_predictions_prob <- fit_knn |> 
  predict(typing_test, 
          type = c("prob")) |>
  cbind(typing_test$y) |>
  mutate(y = typing_test$y)

knn_roc <- knn_test_predictions_prob |>
  roc_curve(truth = y,
            `.pred_1`,
            event_level = "second")

knn_roc |> 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() 
```

Does it outperform a random forest fit to the same training data?

```{r}
library(ranger)
rf_parsnip <- rand_forest() |> 
  set_mode("classification") |>
  set_engine("ranger")

rf_workflow <- workflow() |>
  add_model(rf_parsnip) |>
  add_formula(y ~ .) |>
  step_normalize(all_predictors())

fit_rf <- rf_workflow |>
  fit(typing_train)

fit_rf |>
  extract_fit_engine()
```

```{r}
rf_test_predictions <- fit_rf |>
  predict(typing_test) |>
  cbind(typing_test$y) |>
  mutate(y = typing_test$y)

rf_test_predictions |>
  conf_mat(truth = y, estimate = .pred_class)
```

```{r}
rf_test_predictions_prob <- fit_rf |> 
  predict(typing_test, 
          type = c("prob")) |>
  cbind(typing_test$y) |>
  mutate(y = typing_test$y)

rf_roc <- rf_test_predictions_prob |>
  roc_curve(truth = y,
            `.pred_1`,
            event_level = "second")

rf_roc |> 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() 
```

Though the confusion matrix shows that the KNN model might be a more accurate classifier, the ROC curve shows that the random forest model makes better probabilistic predictions.