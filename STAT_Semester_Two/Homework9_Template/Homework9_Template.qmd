---
title: "Homework 9 Template"
author: YOUR NAME HERE
format: 
  html:
    embed-resources: true
---

The goal of this week's homework is to practice fitting models using workflows. For our analysis, I have modified the data from the schrute package. 

These new data are provided on HuskyCT as theoffice_new.Rda.

```{r}
#| message: FALSE
#| warning: FALSE
library(tidyverse)
library(tidymodels)
theme_set(theme_bw())
tidymodels_prefer()
load("theoffice_new.Rda")

theoffice_new |>
  glimpse()
```

Each of the 186 rows in this dataset corresponds an episode of The Office. There 85 columns (variables) described as follows: 

- season: a factor describing the season number of the episode.

- episode: an integer describing its episode number placement within the season.

- season_percentage: derived from episode. It is a number within [0,1] describing how far along in the season the episode occurs. For instance, the final season for each episode is 1 [100 percent of the way through the season], the first episode is near 1/(Total number of episodes in the season).

- imdb_rating: the average numerical score [out of 10] of this episode as rated by imdb users. This variable will be our response variable.

- director: a factor describing the director of the episode.

- dialog_share_(character) variables: a group of variables describing the percentage of lines of dialog spoken by the character (or group of characters) in an episode. Characters who had dialog in four or fewer episodes of the series are pooled into an "other" category. 

- writer_(name) variables: variables indicating whether or not the named writer is credited for the episode. Writers credited with four or fewer episodes are grouped together as "other". If more than one "other" writers is credited on an episode, the writer_other variable will give the count of "other" writers credited.

If you would like to see the code I used to modify this data from the form it was provided in the schrute package, I have provided it as theoffice_new.R on HuskyCT.

# Problem 1 [No credit. Just run the code after loading the data.]

Let's begin by doing a test-train split of the data.

```{r}
set.seed(12)
theoffice_split <- initial_split(theoffice_new, 
                                 prop = 0.8)

theoffice_train <- theoffice_split |>
  training()

theoffice_test <- theoffice_split |>
  testing()
```

We will use this test-train split going forward.

## Problem 2 [85 Points Total]

### Part A [20 points]

Using a workflow and the training data, fit a least squares linear regression model with imdb_rating as the response and the season factor as the only predictor. 

```{r}
fit_lm <- linear_reg() |> 
  set_engine("lm") |> 
  set_mode("regression")
```

```{r}
leastsquares_workflow <- workflow()
```

```{r}
leastsquares_workflow <- leastsquares_workflow |>
  add_model(fit_lm) |>
  add_formula(imdb_rating ~ season)
```

According to your model fit, which season is estimated to have the highest positive rating effect? 

```{r}
fit_lm <- leastsquares_workflow |>
  fit(theoffice_train)

fit_lm |>
  tidy()

#Season 4
```

Which season is estimated to have the lowest rating effect?

```{r}
# Season 8
```

Visualize the effects of each season. It should resemble the following.

```{r}
fit_lm |>
  tidy() |>
   filter(str_detect(term, "(Intercept)") == FALSE) |>
  ggplot(aes(x = estimate,
             y = term)) +
  geom_col()
```

Why isn't there an estimated coefficient for Season 1?

Season 1 is represented by the intercept. The estimate for season n is the numeric difference between season n's imdb rating and season 1's imdb rating.

### Part B [25 points]

Using a recipe-based workflow and the training data, fit a LASSO linear regression model with imdb_rating as the response and all remaining variables as predictors. Use a lasso penalty of 0.01.

The recipe should consist of three steps in the following order.

- For the director variable, create an "other" category for all directors who directed 2 percent or fewer of the episodes. step_other() should be used for this.

- step_dummy() to turn all nominal predictors into factors.

- step_normalize() to normalize all the predictors.

```{r}
fit_glmnet <- linear_reg(penalty = 0.01) |>
  set_engine("glmnet") |> 
  set_mode("regression")
```

```{r}
recipe_office <- recipe(data = theoffice_train,
                      formula = imdb_rating ~ .) 

recipe_office <- recipe_office |>
  step_other(director, 
             threshold = 0.02,
             other = "other") |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_predictors())
```

```{r}
recipe_workflow_glm <- workflow()

recipe_workflow_glm <- recipe_workflow_glm |>
  add_model(fit_glmnet) |>
  add_recipe(recipe_office)

recipe_workflow_glm
```

```{r}
fit_glm <- recipe_workflow_glm |>
  fit(theoffice_train)

fit_glm |>
  tidy()
```

In this model, which character's dialog is estimated to have the highest positive effect on episode rating? 

```{r}
fit_glm |>
  tidy() |>
  filter(str_starts(term, "dialog")) |>
  arrange(desc(estimate))

# Michael
```

Which writer is estimated to have the highest positive effect on episode rating?

```{r}
fit_glm |>
  tidy() |>
  filter(str_starts(term, "writer")) |>
  arrange(desc(estimate))

# Greg Daniels
```

Which director is estimated to have the lowest effect on episode rating?

```{r}
fit_glm |>
  tidy() |>
  filter(str_starts(term, "director")) |>
  arrange(estimate)

# Randall Einhorn
```

### Part C [20 points]

Using a workflow and the training data, fit a random forest model with imdb_rating as the response and all remaining variables as predictors.

```{r}
parsnip_rf <- rand_forest() |>
  set_mode("regression") |>
  set_engine("ranger")

recipe_office <- recipe(data = theoffice_train,
                      formula = imdb_rating ~ .) 

recipe_office <- recipe_office |>
  step_other(director, 
             threshold = 0.02,
             other = "other") |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_predictors())

recipe_workflow <- workflow()

recipe_workflow <- recipe_workflow |>
  add_model(parsnip_rf) |>
  add_recipe(recipe_office)

fit_rf <- recipe_workflow |>
  fit(theoffice_train)

fit_rf |>
  extract_fit_engine() 
```

Which episode in the test set does this random forest model predict will have the highest rating?

```{r}
rating_predictions <- fit_rf |>
  predict(theoffice_train)

office_train_with_predictions <- rating_predictions |>
  bind_cols(theoffice_train) |>
  select(season, episode, .pred) |>
  arrange(desc(.pred))

office_train_with_predictions

# S9 E24
```

Which episode in the test set does this random forest model predict will have the lowest rating?

```{r}
office_train_with_predictions |>
  arrange(.pred)

# S8 E19
```

### Part D [20 points]

Using a workflow and the training data, fit a k-nearest neighbors model with imdb_rating as the response and all remaining variables as predictors. Use 3 neighbors. Hint: you'll need to load the kknn library.

```{r}
library(kknn)

parsnip_knn <- nearest_neighbor(weight_func = "rectangular", neighbors = 5) |>
  set_engine("kknn") |>
  set_mode("regression")

recipe_office <- recipe(data = theoffice_train,
                      formula = imdb_rating ~ .) 

recipe_office <- recipe_office |>
  step_other(director, 
             threshold = 0.02,
             other = "other") |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_predictors())

recipe_workflow <- workflow()

recipe_workflow <- recipe_workflow |>
  add_model(parsnip_knn) |>
  add_recipe(recipe_office)

fit_knn <- recipe_workflow |>
  fit(theoffice_train)

fit_knn |>
  extract_fit_engine()
```

How many episodes in the test set does this model predict to have average ratings scores above 8?

```{r}
rating_predictions <- fit_knn |>
  predict(theoffice_train)

office_train_with_predictions <- rating_predictions |>
  bind_cols(theoffice_train) |>
  select(season, episode, .pred) |>
  filter(.pred > 8) |>
  summarize(row_count = n())

office_train_with_predictions

# 120
```

How many does it predict will have a score below 7.5?

```{r}
rating_predictions <- fit_knn |>
  predict(theoffice_train)

office_train_with_predictions <- rating_predictions |>
  bind_cols(theoffice_train) |>
  select(season, episode, .pred) |>
  filter(.pred < 7.5) |>
  summarize(row_count = n())

office_train_with_predictions

# 0
```

## Problem 3 [15 points]

For each of the four models in part B, generate predictions for all of the episodes in the test set.

```{r}
rating_predictions_knn <- fit_knn |>
  predict(theoffice_test) |>
  bind_cols(theoffice_test) |>
  mutate(prediction = "knn")

rating_predictions_lm <- fit_lm |>
  predict(theoffice_test) |>
  bind_cols(theoffice_test) |>
  mutate(prediction = "lm")

rating_predictions_glm <- fit_glm |>
  predict(theoffice_test) |>
  bind_cols(theoffice_test) |>
  mutate(prediction = "lasso")

rating_predictions_rf <- fit_rf |>
  predict(theoffice_test) |>
  bind_cols(theoffice_test) |>
  mutate(prediction = "rf")
```

Create a faceted plot showing the predicted value versus the true imdb_rating for each test episode and each model. It should resemble the following---but don't worry if it isn't exactly the same.

Hint: coord_obs_pred() is a helpful setting to add to the plot to make it look nice. Facet 1 is for the model in Part A, Facet 2 is for the model in Part B, etc.

```{r}
rating_predictions <- rbind(rating_predictions_knn, rating_predictions_lm, rating_predictions_glm, rating_predictions_rf)

rating_predictions |>
  ggplot(aes(x = imdb_rating, y = .pred)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  facet_wrap(~ prediction, nrow = 2) +
  coord_obs_pred()  + 
  labs(y = "Prediction", x = "IMDB rating")
```